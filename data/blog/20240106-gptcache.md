---
title: 'GPTCache'
date: '2024-01-06'
tags: ['GPTCache', 'ai']
images: ['/static/images/social-banner.png']
summary: 'GPTCacheë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì— ì§ˆì˜ë¥¼ í•  ë•Œ, ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” Cacheì— ì €ì¥ëœ ë‹µë³€ ê°’ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•´ë³´ì•˜ë‹¤. Langchainê³¼ Langserveë¥¼ í†µí•´ì„œ ê°„ë‹¨íˆ Cacheë¥¼ ì œê³µí•˜ëŠ” API serverë¥¼ ë§Œë“¤ ìˆ˜ ìˆì—ˆë‹¤. Defaultë¡œ ì„¤ì •ëœ GPTCacheì—ì„œ ì˜ë¯¸ì ìœ¼ë¡œ ì¶©ë¶„íˆ ë‹¤ë¥¸ ì§ˆì˜ì— ëŒ€í•´ì„œë„ ê¸°ì¡´ Cacheê°’ì„ ì‚¬ìš©í•˜ëŠ” False Positive ê²°ê³¼ë¥¼ ë°›ì•˜ë‹¤. ê·¸ë˜ì„œ Defaultë¡œ ì‚¬ìš©ëœ Similarity Evaluation ë°©ë²•ì´ ì–´ë–»ê²Œ ë™ì‘ë˜ëŠ”ì§€ ì‚´í´ë³´ì•˜ë‹¤. ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Thresholdê°’ì„ ë³€ê²½í•˜ì—¬ ë°œìƒí–ˆë˜ False Positive caseë¥¼ ì œê±°í•´ë³´ì•˜ë‹¤. ì•ìœ¼ë¡œ Cache hit rate, Accuracy, Speedë¥¼ ê³ ë ¤í•˜ì—¬ ì„œë¹„ìŠ¤ì— ì í•©í•œ Evaluation ë°©ë²•ì„ ì„ íƒí•˜ê¸° ìœ„í•´ì„œëŠ” Default ë§ê³  ë‹¤ë¥¸ ì˜µì…˜ë“¤ë„ í™•ì¸í•  í•„ìš”ê°€ ìˆê² ë‹¤.'
---

## Semantic Caching

LLMì— ì§ˆì˜ë¥¼ í•  ë•Œ, ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” Cacheë¡œ ë¹ ë¥´ê²Œ ì‘ë‹µí•˜ê³  ì‹¶ì—ˆë‹¤. ì •í™•íˆ ë™ì¼í•œ ë¬¸ìì—´ì— ëŒ€í•´ì„œ Cacheë¥¼ ì´ìš©í•˜ë©´, cache hit rateì´ ìƒë‹¹íˆ ë–¨ì–´ì ¸ì„œ ë¹„íš¨ìœ¨ì ì¸ ê²ƒì´ ìëª…í–ˆë‹¤. `What is the happiest memory?` ë¬¸ìì—´ì— ëŒ€í•´ì„œ Cacheë¥¼ í•˜ê³  ìˆë‹¤ë¼ê³  í•´ë„, `What is happiest memory?`ë¼ê³  the ì „ì¹˜ì‚¬ë¥¼ ë¹¼ë¨¹ìœ¼ë©´ ì˜ë¯¸ëŠ” ë™ì¼í•˜ì§€ë§Œ Cache hitì„ í•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤. ê·¸ë˜ì„œ chatGPTë‚˜ Llama 2 ê°™ì€ LLMì„ ì‚¬ìš©í•  ë•Œ, ì–´ë–»ê²Œ cacheë¥¼ í•  ìˆ˜ ìˆì„ì§€ ê¶ê¸ˆí•´ì¡Œë‹¤. ì°¾ì•„ë³´ë‹ˆ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ì¸ [GPTCache](https://github.com/zilliztech/GPTCache)ê°€ ìˆì—ˆê³ , ì•„ë˜ì²˜ëŸ¼ ì„¤ëª…í•˜ê³  ìˆë‹¤.

> However, using an exact match approach for LLM caches is less effective due to the complexity and variability of LLM queries, resulting in a low cache hit rate. To address this issue, GPTCache adopt alternative strategies like semantic caching. Semantic caching identifies and stores similar or related queries, thereby increasing cache hit probability and enhancing overall caching efficiency.

`Semantic caching`ì´ë¼ëŠ” ìš©ì–´ê°€ ë‚˜ì˜¤ëŠ”ë°, ì˜ë¯¸ì ìœ¼ë¡œ ê°™ì€ ê²ƒì„ cacheí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì–´ë–»ê²Œ `Semantic caching`ì´ ë˜ëŠ”ì§€ëŠ” ì•„ë˜ì™€ ê°™ì´ ì„¤ëª…ë˜ì–´ ìˆë‹¤.

> GPTCache employs embedding algorithms to convert queries into embeddings and uses a vector store for similarity search on these embeddings.

LLMì—ê²Œ ì§ˆë¬¸ì„ í•˜ë©´(Query) ê·¸ ì§ˆë¬¸ì„ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ vector í˜•ì‹ì˜ ë°ì´í„°ë¡œ ë§Œë“¤ê³ (Embeddings), ê·¸ê²ƒì„ ì €ì¥ì†Œì— ì €ì¥í•œë‹¤. ì´ì œ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í–ˆì„ ë•Œ ì´ë¯¸ ìœ ì‚¬í•œ ì§ˆë¬¸ì„ í–ˆëŠ”ì§€ë¥¼ íŒë‹¨í•˜ê²Œ ë˜ëŠ”ë°, ì´ê²ƒì€ ìœ ì‚¬í•œ Embeddingì´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ì‘ì—…ì´ ì§„í–‰ëœë‹¤. ìµœì¢…ì ìœ¼ë¡œ ìœ ì‚¬í•œ Embedding ìˆë‹¤ë©´ cacheëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê³ , ì—†ë‹¤ë©´ LLMì—ì„œ ìƒˆë¡œìš´ ë‹µë³€ì„ ìƒì„±í•˜ê²Œ ëœë‹¤.

## Testing with Langchain

GPTCacheëŠ” `Langchain`ì„ ì§€ì›í•˜ê³ , [ë¬¸ì„œì—ì„œ ì‰½ê²Œ GPTCacheë¥¼ ì—°ë™í•˜ëŠ” ë°©ë²•](https://python.langchain.com/docs/integrations/llms/llm_caching#gptcache)ì„ ì„¤ëª…í•˜ê³  ìˆë‹¤. ê·¸ë˜ì„œ `Langchain`ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ë¥¼ í•˜ê²Œ ë˜ì—ˆë‹¤. [Langchain ê³µì‹ ë¬¸ì„œì˜ Quickstart](https://python.langchain.com/docs/get_started/quickstart)ë¥¼ ë”°ë¼ì„œ ì§„í–‰í•˜ì˜€ê³ , Macì—ì„œ [Ollama](https://ollama.ai/download)ë¥¼ ì„¤ì¹˜í•˜ì—¬ ë¡œì»¬ì—ì„œ `llama2` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. `langserve` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ì„œ FastAPI frameworkë¡œ API serverë¥¼ ë§Œë“¤ ìˆ˜ ìˆì—ˆë‹¤.

í•„ìš”í•œ ëª¨ë“ˆë“¤ì„ virtualenvì— ì„¤ì¹˜ë¥¼ í•œë‹¤.

```bash
pyenv virtualenv 3.9 langchain
pyenv activate langchain
pip install langchain
pip install gptcache
pip install "langserve[server]"
```

ê·¸ë¦¬ê³  ì•„ë˜ì²˜ëŸ¼ ì½”ë“œë¥¼ ì‘ì„±í•˜ì—¬ ì‹¤í–‰ì„ í•œë‹¤.

`main.py`

```py
import hashlib
from fastapi import FastAPI
from langserve import add_routes
from gptcache import Cache
from gptcache.adapter.api import init_similar_cache
from langchain.cache import GPTCache
from langchain_community.llms import Ollama
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.globals import set_llm_cache

app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)

def get_hashed_name(name):
    return hashlib.sha256(name.encode()).hexdigest()


def init_gptcache(cache_obj: Cache, llm: str):
    hashed_llm = get_hashed_name(llm)
    init_similar_cache(cache_obj=cache_obj, data_dir=f"similar_cache_{hashed_llm}")

set_llm_cache(GPTCache(init_gptcache))

prompt = ChatPromptTemplate.from_messages([
  ("system", "You are world class technical documentation writer."),
  ("user", "{input}")
])
llm = Ollama(model="llama2")
output_parser = StrOutputParser()
chain = prompt | llm | output_parser

add_routes(
    app,
    chain,
    path="/myllm",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

ì´ì œ localhostì— ì‹¤í–‰ì¤‘ì¸ API ì„œë²„ì— curl ì§ˆì˜ë¥¼ í•´ë³¸ë‹¤. ì²˜ìŒì—ëŠ” ì‹œê°„ì´ ì¢€ ê±¸ë¦¬ì§€ë§Œ, ë‘ë²ˆ ì§¸ ìš”ì²­ë¶€í„°ëŠ” ë”°ë¥´ê²Œ ì‘ë‹µì´ ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```bash
curl --location --request POST 'http://localhost:8000/myllm/invoke' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "input": {
            "input": "what is the best way to learn new languages?"
        }
    }'
```

ê·¸ë¦¬ê³  ì´ì œ ì•„ë˜ì™€ ê°™ì€ query ë¬¸ì¥ì„ í•´ë„ Cache ê°’ìœ¼ë¡œ ë°”ë¡œ ì‘ë‹µí•˜ëŠ” ê²ƒë„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```
what is the best way to learn new languages?
what is best way to learn new languages?
what is best way to run new languages?
what is best way to learn languages?
```

ê·¸ëŸ°ë° ë‹¤ë¥¸ ë‹µë³€ì„ ê¸°ëŒ€í•  ë§Œí•œ ì§ˆë¬¸ì—ì„œë„ ë™ì¼í•œ Cache ê°’ìœ¼ë¡œ ì‘ë‹µí•œë‹¤. False positive ê²°ê³¼ë„ ì‰½ê²Œ í™•ì¸ì´ ë˜ì—ˆë‹¤.

```
what is the best way to learn Korean?
Where is the best place to study?
How do you study math?
```

ì´ì œ ì™„ì „ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•˜ë©´ LLMìœ¼ë¡œë¶€í„° ìƒˆë¡­ê²Œ ë‹µë³€ì„ ìƒì„±í•˜ê³ , ê·¸ë‹¤ìŒì— ìœ ì‚¬í•œ ì§ˆë¬¸ì—ëŠ” Cache ê°’ì„ ë‹¤ì‹œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.

```
what is embeddings?
please explain embeddings?
```

## False Positive

ìœ„ì—ì„œ `How do you study math?`ë¥¼ í–ˆì„ ë•Œë„ `what is the best way to learn new languages?`ì˜ ëŒ€ë‹µ Cache ê°’ì„ ì „ë‹¬í•˜ì˜€ë‹¤. `How do you study math?`ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ìƒˆë¡œ LLMì—ì„œ ì‘ë‹µì„ ê°€ì ¸ì˜¤ê¸¸ ê¸°ëŒ€í–ˆëŠ”ë°, Cache ê°’ì„ ê°€ì ¸ì˜¨ ê²ƒì´ë‹¤. ğŸ§ ê·¸ë˜ì„œ ì–´ë–»ê²Œ Cacheê°€ ë˜ëŠ” ì¡°ê±´ì„ ì¡°ì ˆ í•  ìˆ˜ ìˆì„ì§€ ë” ìì„¸íˆ ì•Œì•„ë³´ê²Œ ë˜ì—ˆë‹¤.

### init_similar_cacheì˜ default ì„¤ì •

[GPTCache ë¬¸ì„œì—ì„œ init_similar_cache methodì˜ ì¸ì ê°’ì„ ì–´ë–»ê²Œ ì¤„ ìˆ˜ ìˆëŠ”ì§€ ì„¤ëª…](https://gptcache.readthedocs.io/en/latest/configure_it.html#introduction-to-gptcache-initialization)ì´ ë˜ì–´ ìˆë‹¤. ì´ ë¬¸ì„œì—ì„œ defaultë¡œ `onnx+sqlite+faiss`ë¼ê³  ì„¤ëª…ë˜ì–´ ìˆë‹¤.

> The init_similar_cache method in the api package defaults to similar matching of onnx+sqlite+faiss

[GPTCache sourceì½”ë“œì—ì„œ í•´ë‹¹ methodë¥¼ í™•ì¸](https://github.com/zilliztech/GPTCache/blob/main/gptcache/adapter/api.py)í•´ë³´ì•˜ë‹¤. ê·¸ëŸ¬ë‹ˆ ê° êµ¬ì„±ìš”ì†Œê°€ ì•„ë˜ì²˜ëŸ¼ defaultë¡œ ì„¤ì •ë˜ì–´ ìˆì—ˆë‹¤.

- embedding: `Onnx`
- data_manage: `sqlite,faiss`
- evaluation: `SearchDistanceEvaluation`

```py
def init_similar_cache(
    data_dir: str = "api_cache",
    cache_obj: Optional[Cache] = None,
    pre_func: Callable = get_prompt,
    embedding: Optional[BaseEmbedding] = None,
    data_manager: Optional[DataManager] = None,
    evaluation: Optional[SimilarityEvaluation] = None,
    post_func: Callable = temperature_softmax,
    config: Config = Config(),
):
    if not embedding:
        embedding = Onnx()
    if not data_manager:
        data_manager = manager_factory(
            "sqlite,faiss",
            data_dir=data_dir,
            vector_params={"dimension": embedding.dimension},
        )
    if not evaluation:
        evaluation = SearchDistanceEvaluation()
    cache_obj = cache_obj if cache_obj else cache
    cache_obj.init(
        pre_embedding_func=pre_func,
        embedding_func=embedding.to_embeddings,
        data_manager=data_manager,
        similarity_evaluation=evaluation,
        post_process_messages_func=post_func,
        config=config,
    )
```

ê·¸ë ‡ê¸° ë•Œë¬¸ì— `main.py`ë¥¼ ì‹¤í–‰í–ˆë‹¤ë©´, `similar_cache_{hashê°’}` ë””ë ‰í„°ë¦¬ ì•ˆì—ëŠ” ì•„ë˜ì™€ ê°™ì´ ë‘ê°œì˜ íŒŒì¼ì´ ìƒì„±ëœë‹¤. defaultë¡œ ì €ì¥ì†Œë¥¼ sqlite3ê°€ ì§€ì •ë˜ì–´ ìˆê³ , vector storeë¡œëŠ” faissê°€ ì„¤ì •ë˜ì–´ ìˆë‹¤.

- sqlite.db
- faiss.index

[ë¬¸ì„œì—ì„œ embeddings ì¢…ë¥˜ë“¤ì— ëŒ€í•œ ì„¤ëª…ê³¼ ì˜ˆì œ](https://gptcache.readthedocs.io/en/latest/references/embedding.html) ì˜ ì •ë¦¬ ë˜ì–´ ìˆë‹¤. defaultë¡œ ì„¤ì •ëœ `Onnx`ëŠ” ì•„ë˜ì²˜ëŸ¼ Onnx modelì„ í†µí•´ì„œ Embeddingì„ ë§Œë“¤ê²Œ ëœë‹¤.

```py
from gptcache.embedding import Onnx

test_sentence = 'Hello, world.'
encoder = Onnx(model='GPTCache/paraphrase-albert-onnx')
embed = encoder.to_embeddings(test_sentence)
print(embed)
```

evaluationì€ `SearchDistanceEvaluation`ë¡œ defaultë¡œ ì„¤ì •ì´ ë˜ì—ˆëŠ”ë°, ì´ì— ëŒ€í•œ ì„¤ëª…ì€ [ë¬¸ì„œ](https://gptcache.readthedocs.io/en/latest/references/similarity_evaluation.html#module-gptcache.similarity_evaluation.distance)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  [Cache ì„¤ì •ì„ ì˜ í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ë¬¸ì„œ](https://gptcache.readthedocs.io/en/latest/configure_it.html#introduction-to-gptcache-initialization)ì—ì„œëŠ” ê° Similarity Evaluation ì¢…ë¥˜ë³„ë¡œ íŠ¹ì§•ì„ ì„¤ëª…í•˜ê³  ìˆë‹¤. `SearchDistanceEvaluation`ëŠ” ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ëŠ” ìƒë‹¹íˆ ë–¨ì–´ì§„ë‹¤.

> SearchDistanceEvaluation, vector search distance, simple, fast, but not very accurate

`SearchDistanceEvaluation`ëŠ” searchí•œ ê²°ê³¼ê°’ì˜ scoreë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤. ê·¸ë˜ì„œ defaultë¡œ ì„¤ì •ëœ `Faiss`ì˜ ê²€ìƒ‰ê²°ê³¼ì— ë”°ë¼ì„œ scoreê°€ ê²°ì •ë˜ê²Œ ëœë‹¤. `Faiss`ì˜ ê²°ìƒ‰ê²°ê³¼ì— ëŒ€í•˜ ì˜ˆì œ ì½”ë“œëŠ” ì•„ë˜ì²˜ëŸ¼ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.

1. `cached_msg_1`ëŠ” GPTCacheì— LLM ì‘ë‹µê°’ì´ Cacheë˜ì–´ ìˆë‹¤. Vector storeëŠ” Faissë¡œ ì„¤ì •ë˜ì–´ ìˆë‹¤.
2. `query_msg_1`ëŠ” Cacheë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë˜ Query ê°’ì„ ì„¤ì •í•˜ì˜€ë‹¤.
3. `query_msg_2`ëŠ” False Positive ê²°ê³¼ê°€ ë‚˜ì™”ë˜ Query ê°’ì„ ì„¤ì •í•˜ì˜€ë‹¤.
4. ì´ì œ Onnx ëª¨ë¸ë¡œ Embeddingë“¤ì„ ë§Œë“¤ê³ , `cached_msg_1`ì˜ embeddingì€ `Faiss`ì— ì¶”ê°€í•œë‹¤.
5. `query_msg_1`ì˜ embeddingìœ¼ë¡œ `Faiss` searchë¥¼ í•´ì„œ ê°€ì¥ ê°€ê¹Œìš´ ê°’ì„ ê°€ì ¸ì˜¨ë‹¤.
6. `query_msg_2`ì˜ embeddingìœ¼ë¡œ `Faiss` searchë¥¼ í•´ì„œ ê°€ì¥ ê°€ê¹Œìš´ ê°’ì„ ê°€ì ¸ì˜¨ë‹¤.

```py
from gptcache.similarity_evaluation import KReciprocalEvaluation
from gptcache.similarity_evaluation import KReciprocalEvaluation
from gptcache.manager.vector_data.faiss import Faiss
from gptcache.manager.vector_data.base import VectorData
import numpy as np
from gptcache.embedding import Onnx

cached_msg_1 = '''
System: You are world class technical documentation writer.
Human: how is the best way to learn new languages?
'''

query_msg_1 = '''
System: You are world class technical documentation writer.
Human: what is embeddings?
'''

query_msg_2 = '''
System: You are world class technical documentation writer.
Human: How do you study math?
'''

encoder = Onnx(model='GPTCache/paraphrase-albert-onnx')
cached = encoder.to_embeddings(cached_msg_1)
query1 = encoder.to_embeddings(query_msg_1)
query2 = encoder.to_embeddings(query_msg_2)
faiss = Faiss('./none', encoder.dimension, 10)
cached_magnitude = np.linalg.norm(cached)
query1_magnitude = np.linalg.norm(query1)
query2_magnitude = np.linalg.norm(query2)
faiss.mul_add([VectorData(id=0, data=cached / cached_magnitude)])
print(faiss.search(query1 / query1_magnitude, 1))
print(faiss.search(query2 / query2_magnitude, 1))
```

ê²°ê³¼ëŠ” `what is embeddings?`ì˜ ê²½ìš°ì—ëŠ” `0.98192865`ê°’ì´ ë‚˜ì™”ê³ , `How do you study math?`ì˜ ê²½ìš°ì—ëŠ” `0.700819` ê°’ì´ ë‚˜ì™”ë‹¤. [GPTcache source code](https://github.com/zilliztech/GPTCache/blob/acc20f05400dabdcde451194e9bb73b986747685/gptcache/config.py#L11)ë¥¼ ì‚´í´ë³´ë‹ˆ `similarity_threshold` ê°’ì´ `0.8`ë¡œ ì„¤ì •ë˜ì–´ ìˆë‹¤. ë”°ë¼ì„œ `How do you study math?`ëŠ” thresholdë³´ë‹¤ ì‘ê¸° ë•Œë¬¸ì— í•´ë‹¹ Cache ê°’ì„ ì‚¬ìš©í•œ ê²ƒì´ë‹¤.

```bash
[(0.98192865, 0)]
[(0.700819, 0)]
```

Cache ê°’ì´ ì—¬ëŸ¬ê°œì¼ ë•ŒëŠ” `Faiss`ì—ì„œ searchí•  ë•Œ ê°€ì¥ ê°€ê¹Œìš´ ê°’ë¶€í„° ê°€ì ¸ì˜¤ê²Œ ëœë‹¤. ì˜ˆì œ ì½”ë“œë¥¼ ì•„ë˜ì²˜ëŸ¼ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.

1. Faissì— ë‘ê°œì˜ vector ê°’ì´ ì¡´ì¬í•˜ëŠ” í™˜ê²½ì„ êµ¬ì„±í•œë‹¤. faiss.mul_addë¥¼ í†µí•´ì„œ ë‘ê°œì˜ vectorê°’ì„ ë‹¤ë¥¸ idë¡œ ì €ì¥í•œë‹¤.
2. Faiss id 1ì— ë” ì˜ë¯¸ì ìœ¼ë¡œ ê°€ê¹Œìš´ `please explain embeddings?` ê°’ìœ¼ë¡œ search ê²°ê³¼ë¥¼ í™•ì¸í•´ë³¸ë‹¤.

```py
from gptcache.similarity_evaluation import KReciprocalEvaluation
from gptcache.similarity_evaluation import KReciprocalEvaluation
from gptcache.manager.vector_data.faiss import Faiss
from gptcache.manager.vector_data.base import VectorData
import numpy as np
from gptcache.embedding import Onnx

cached_msg_1 = '''
System: You are world class technical documentation writer.
Human: how is the best way to learn new languages?
'''

cached_msg_2 = '''
System: You are world class technical documentation writer.
Human: what is embeddings?
'''

query_msg = '''
System: You are world class technical documentation writer.
Human: please explain embeddings?
'''

encoder = Onnx(model='GPTCache/paraphrase-albert-onnx')
cached_1 = encoder.to_embeddings(cached_msg_1)
cached_2 = encoder.to_embeddings(cached_msg_2)
new = encoder.to_embeddings(query_msg)
faiss = Faiss('./none', encoder.dimension, 10)
magnitude1 = np.linalg.norm(cached_1)
magnitude2 = np.linalg.norm(cached_2)
new_magnitude = np.linalg.norm(new)
faiss.mul_add([VectorData(id=0, data=cached_1 / magnitude1)])
faiss.mul_add([VectorData(id=1, data=cached_2 / magnitude2)])
print(faiss.search(new / new_magnitude, 1))
```

`faiss.search(new / new_magnitude, 1)`ì—ì„œ 1ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ê°’ í•˜ë‚˜ë§Œ ë³´ì—¬ì£¼ë„ë¡ í–ˆê¸° ë•Œë¬¸ì— ì•„ë˜ì²˜ëŸ¼ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤. Tupleì—ì„œ ë‘ë²ˆì§¸ ê°’ì€ idê°’ì„ ë‚˜íƒ€ë‚´ê³ , idê°€ ê¸°ëŒ€í•œê²ƒì²˜ëŸ¼ 1ë¡œ ë‚˜ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```bash
[(0.15015657, 1)]
```

### similarity_threshold ì ìš©

defaultì—ì„œ ì–´ë–»ê²Œ evaluationì´ ì§„í–‰ë˜ì–´ì„œ scoreê°€ ë‚˜ì˜¤ê³ , threshold ê°’ì„ í†µí•´ì„œ Cache ê°’ì„ ì‚¬ìš©í• ì§€ ë§ì§€ ê²°ì •í•˜ëŠ” ë¡œì§ì„ í™•ì¸í•˜ì˜€ë‹¤. ì´ì œ langserveë¡œ êµ¬ì„±í•œ API serverì— ì•„ë˜ì™€ ê°™ì´ `Config(similarity_threshold=0.1)`ë¥¼ ì ìš©í•˜ì—¬ ìœ„ì—ì„œ ë°œìƒí•œ False positive ë°œìƒì„ ê°œì„ í•  ìˆ˜ ìˆë‹¤. Cache hit rateëŠ” ì¤„ì–´ë“¤ê² ì§€ë§Œ, ë” ìœ ì‚¬ë„ê°€ ë†’ì€ Queryë§Œ Cache ê°’ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.

```py
import hashlib
from fastapi import FastAPI
from langserve import add_routes
from gptcache import Cache, Config
from gptcache.adapter.api import init_similar_cache
from langchain.cache import GPTCache
from langchain_community.llms import Ollama
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.globals import set_llm_cache

app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)

def get_hashed_name(name):
    return hashlib.sha256(name.encode()).hexdigest()


def init_gptcache(cache_obj: Cache, llm: str):
    hashed_llm = get_hashed_name(llm)
    init_similar_cache(cache_obj=cache_obj, data_dir=f"similar_cache_{hashed_llm}", config=Config(similarity_threshold=0.1))

set_llm_cache(GPTCache(init_gptcache))

prompt = ChatPromptTemplate.from_messages([
  ("system", "You are world class technical documentation writer."),
  ("user", "{input}")
])
llm = Ollama(model="llama2")
output_parser = StrOutputParser()
chain = prompt | llm | output_parser

add_routes(
    app,
    chain,
    path="/myllm",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

## sqlite3 ë°ì´í„° í™•ì¸

Sqlite3ì— ìƒì„±ëœ í…Œì´ë¸” êµ¬ì¡°ì™€ ë°ì´í„°ë„ í™•ì¸í•´ë³´ì•˜ë‹¤.

```bash
sqlite3 sqlite.db
```

í…Œì´ë¸”ì„ ì•„ë˜ì™€ ê°™ì´ ìƒì„±ëœë‹¤.

```bash
sqlite> .tables
gptcache_answer        gptcache_question_dep  gptcache_session
gptcache_question      gptcache_report
```

`gptcache_question` í…Œì´ë¸”ì— `embedding_data`ë¼ëŠ” columnì´ ìˆê³ , ì´ì œ Queryê°€ Embeddingìœ¼ë¡œ ë³€í™˜ë˜ì–´ì„œ binaryë¡œ ì €ì¥ë˜ê³  ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```bash
sqlite> .schema gptcache_question
CREATE TABLE gptcache_question (
        id INTEGER NOT NULL,
        question VARCHAR(3000) NOT NULL,
        create_on DATETIME,
        last_access DATETIME,
        embedding_data BLOB,
        deleted INTEGER,
        PRIMARY KEY (id)
);
```

ê·¸ë¦¬ê³  `gptcache_report` í…Œì´ë¸”ì—ì„œëŠ” `similarity`ë¼ëŠ” columnì´ ìˆê³ , ì´ê²ƒì€ ìœ ì‚¬ë„ ê°’ì„ ë‹´ê³  ìˆë‹¤. ìœ„ì—ì„œ `How do you study math?`ë„ ë™ì¼í•œ ë‹µë³€ì„ ì–»ì—ˆëŠ”ë°, ì´ `similarity` ê°’ì´ 3.2ì´ë‹¤. `what is best way to run new languages?` learnì„ runìœ¼ë¡œ ì˜ëª» ì ì€ ê²½ìš°ì—ëŠ” ì´ ê°’ì´ 3.98ì´ë‹¤. distance_maxê°€ defaultë¡œ 4ë¡œ ì„¤ì •ì´ ë˜ì–´ ìˆê³ , `Faiss` searchë¡œ ê°€ì ¸ì˜¨ ê²°ê³¼ë¥¼ ëº€ ê²°ê³¼ì´ë‹¤. `getcache_report` í…Œì´ë¸” ê°’ì„ í†µí•´ì„œ Queryë§ˆë‹¤ ì–´ë–¤ ìœ ì‚¬ë„ë¡œ ì–´ë–¤ cache ê°’ì„ ì‚¬ìš©í–ˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```bash
sqlite> .schema gptcache_report
CREATE TABLE gptcache_report (
        id INTEGER NOT NULL,
        user_question VARCHAR(3000) NOT NULL,
        cache_question_id INTEGER NOT NULL,
        cache_question VARCHAR(3000) NOT NULL,
        cache_answer VARCHAR(3000) NOT NULL,
        similarity FLOAT NOT NULL,
        cache_delta_time FLOAT NOT NULL,
        cache_time DATETIME,
        extra VARCHAR(3000),
        PRIMARY KEY (id)
);
```

## Faiss segmentation fault with torch

torchë¥¼ ì„¤ì¹˜í•˜ê³  Faissë¥¼ ì‚¬ìš©í•´ì„œ GPTCacheë¥¼ ì‚¬ìš©í•  ë•Œ, `Segmentation fault`ê°€ ë°œìƒí•˜ì˜€ë‹¤. `torch`ëŠ” 2.1.2 ë²„ì „ì„ ì‚¬ìš©í•˜ê³ , `faiss-cpu` 1.7.4 ë²„ì „ì„ ì‚¬ìš©í•˜ê³  ìˆì—ˆë‹¤. ì™œ Segmentation faultê°€ ë‚˜ëŠ”ì§€ ì´í•´í•  ìˆ˜ê°€ ì—†ì—ˆê³ , ì‚½ì§ˆì„ í•œ ëì— í˜¹ì‹œë‚˜ í•˜ëŠ” ë§ˆìŒì— `faiss-cpu`ë¥¼ 1.7.0ìœ¼ë¡œ ì„¤ì¹˜í•˜ì—¬ ì‹¤í–‰í–ˆë‹¤. ê·¸ë¬ë”ë‹ˆ ì´ì œ í•´ë‹¹ ì—ëŸ¬ ì—†ì´ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ì´ ë˜ì—ˆë‹¤.ğŸ˜­

## HuggingFace

[langchainì—ì„œ HuggingFace Local Pipelineë„ ì—°ë™](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines)í•  ìˆ˜ ìˆë‹¤. CPUì—ì„œ ì—„ì²­ ì˜¤ë˜ ê±¸ë ¸ì§€ë§Œ ì‘ë‹µì„ ì •ìƒì ìœ¼ë¡œ ë°›ì•˜ë‹¤.

```py
import os
from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

model_id = "tiiuae/falcon-7b-instruct"
model = AutoModelForCausalLM.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)

pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
    max_length=5000,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
)

llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})
template = """
You are an ethical hacker and programmer. Help the following question with brilliant answers.
Question: {question}
Answer:"""
prompt = PromptTemplate(template=template, input_variables=["question"])

llm_chain = LLMChain(prompt=prompt, llm=llm)

question = "Create a python script to send a DNS packet using scapy with a secret payload "

print(llm_chain.invoke(question))
```

## ê²°ë¡ 

`Langchain`ê³¼ `GPTCache`ë¥¼ í†µí•´ì„œ ê°„ë‹¨í•˜ê²Œ LLMìœ¼ë¡œ ì„œë¹„ìŠ¤í•˜ëŠ” API ì„œë²„ë¥¼ êµ¬ì„±í•˜ê³ , Semantic cachingì„ ì ìš©í•  ìˆ˜ ìˆì—ˆë‹¤. ê·¸ ê³¼ì •ì—ì„œ Embeddings, Faiss ë“±ì„ ìƒˆë¡œ ì•Œê²Œ ë˜ì—ˆë‹¤.
