---
title: 'GPTCache'
date: '2024-01-06'
tags: ['GPTCache', 'ai']
images: ['/static/images/social-banner.png']
summary: ''
---

## Semantic Caching

생성형 AI에게 질문을 할 때, 문자열 그대로 동일한 질문에 대해서 cache를 한다면 cache hit rate이 상당히 떨어져서 비효율적인 것이 자명했다. 그래서 chatGPT나 Llama 2 같은 LLM을 사용할 때, 어떻게 cache를 할 수 있을지 궁금해졌다. 찾아보니 오픈소스 프로젝트인 [GPTCache](https://github.com/zilliztech/GPTCache)가 있었고, 아래처럼 설명하고 있다.

> However, using an exact match approach for LLM caches is less effective due to the complexity and variability of LLM queries, resulting in a low cache hit rate. To address this issue, GPTCache adopt alternative strategies like semantic caching. Semantic caching identifies and stores similar or related queries, thereby increasing cache hit probability and enhancing overall caching efficiency.

`Semantic caching`이라는 용어가 나오는데, 의미적으로 같은 것을 cache할 수 있다는 것을 의미한다. 어떻게 `Semantic caching`이 되는지는 아래와 같이 설명되어 있다.

> GPTCache employs embedding algorithms to convert queries into embeddings and uses a vector store for similarity search on these embeddings.

LLM에게 질문을 하면(Query) 그 질문을 컴퓨터가 이해할 수 있도록 vector 형식의 데이터로 만들고(Embeddings), 그것을 저장소에 저장한다. 이제 다른 질문을 했을 때 이미 유사한 질문을 했는지를 판단하게 되는데, 이것은 유사한 Embedding이 있는지 확인하는 작업이 진행된다. 최종적으로 유사한 Embedding 있다면 cache된 데이터를 사용하고, 없다면 LLM에서 새로운 답변을 생성하게 된다.

## Testing with Langchain

GPTCache는 `Langchain`을 지원하고, [문서에서 쉽게 GPTCache를 연동하는 방법](https://python.langchain.com/docs/integrations/llms/llm_caching#gptcache)을 설명하고 있다. 그래서 `Langchain`으로 사용하여 테스트를 하게 되었다. [Langchain 공식 문서의 Quickstart](https://python.langchain.com/docs/get_started/quickstart)를 따라서 진행하였고, Mac에서 [Ollama](https://ollama.ai/download)를 설치하여 로컬에서 `llama2` 모델을 사용하였다. `langserve` 라이브러리를 통해서 FastAPI framework로 API server를 만들 수 있었다.

필요한 모듈들을 virtualenv에 설치를 한다.

```bash
pyenv virtualenv 3.9 langchain
pyenv activate langchain
pip install langchain
pip install gptcache
pip install "langserve[server]"
```

그리고 아래처럼 코드를 작성하여 실행을 한다.

```py
import hashlib
from fastapi import FastAPI
from langserve import add_routes
from gptcache import Cache
from gptcache.adapter.api import init_similar_cache
from langchain.cache import GPTCache
from langchain_community.llms import Ollama
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.globals import set_llm_cache

app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple api server using Langchain's Runnable interfaces",
)

def get_hashed_name(name):
    return hashlib.sha256(name.encode()).hexdigest()


def init_gptcache(cache_obj: Cache, llm: str):
    hashed_llm = get_hashed_name(llm)
    init_similar_cache(cache_obj=cache_obj, data_dir=f"similar_cache_{hashed_llm}")

set_llm_cache(GPTCache(init_gptcache))

prompt = ChatPromptTemplate.from_messages([
  ("system", "You are world class technical documentation writer."),
  ("user", "{input}")
])
llm = Ollama(model="llama2")
output_parser = StrOutputParser()
chain = prompt | llm | output_parser

add_routes(
    app,
    chain,
    path="/myllm",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

이제 localhost에 실행중인 API 서버에 curl 질의를 해본다. 처음에는 시간이 좀 걸리지만, 두번 째 요청부터는 따르게 응답이 오는 것을 확인할 수 있다.

```bash
curl --location --request POST 'http://localhost:8000/myllm/invoke' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "input": {
            "input": "what is the best way to learn new languages?"
        }
    }'
```

그리고 이제 아래와 같은 query 문장을 해도 Cache 값으로 바로 응답하는 것도 확인할 수 있다.

```
what is the best way to learn new languages?
what is best way to learn new languages?
what is best way to run new languages?
what is best way to learn languages?
```

그런데 다른 답변을 기대할 만한 질문에서도 동일한 Cache 값으로 응답한다. False positive 결과도 쉽게 확인이 되었다.

```
what is the best way to learn Korean?
Where is the best place to study?
How do you study math?
```

이제 완전 다른 질문을 하면 LLM으로부터 새롭게 답변을 생성하고, 그다음에 유사한 질문에는 Cache 값을 다시 사용하는 것을 확인할 수 있었다.

```
what is embeddings?
please explain embeddings?
```

## init_similar_cache의 default 설정

`similar_cache_{hash값}` 디렉터리 안에는 아래와 같이 두개의 파일이 생긴다.

- sqlite.db
- faiss.index

먼저 sqlite database에 data를 살펴보기 위해서 sqlite3 명령어로 생성된 table과 schema를 살펴보자.

```bash
sqlite3 sqlite.db
```

```bash
sqlite> .tables
gptcache_answer        gptcache_question_dep  gptcache_session
gptcache_question      gptcache_report
```

`gptcache_question` 테이블에 `embedding_data`라는 column이 있고, 이제 Query가 Embedding으로 변환되어서 binary로 저장되고 있는 것을 확인할 수 있다.

```bash
sqlite> .schema gptcache_question
CREATE TABLE gptcache_question (
        id INTEGER NOT NULL,
        question VARCHAR(3000) NOT NULL,
        create_on DATETIME,
        last_access DATETIME,
        embedding_data BLOB,
        deleted INTEGER,
        PRIMARY KEY (id)
);
```

그리고 `gptcache_report` 테이블에서는 `similarity`라는 column이 있고, 이것은 유사도 값을 담고 있다. 위에서 `How do you study math?`도 동일한 답변을 얻었는데, 이 `similarity` 값이 3.2이다. `what is best way to run new languages?` learn을 run으로 잘못 적은 경우에는 이 값이 3.98이다.

```bash
sqlite> .schema gptcache_report
CREATE TABLE gptcache_report (
        id INTEGER NOT NULL,
        user_question VARCHAR(3000) NOT NULL,
        cache_question_id INTEGER NOT NULL,
        cache_question VARCHAR(3000) NOT NULL,
        cache_answer VARCHAR(3000) NOT NULL,
        similarity FLOAT NOT NULL,
        cache_delta_time FLOAT NOT NULL,
        cache_time DATETIME,
        extra VARCHAR(3000),
        PRIMARY KEY (id)
);
```

[GPTCache 문서에서 init_similar_cache method의 인자 값을 어떻게 줄 수 있는지 설명](https://gptcache.readthedocs.io/en/latest/configure_it.html#introduction-to-gptcache-initialization)이 되어 있다. embedding 파라미터를 설정하지 않으면, default로 onnx+sqlite+faiss를 사용하게 된다. Embedding을 생성할 때 ONNX를 사용하고, 저장소는 Sqlite를 사용하고, 빠른 Similarity 검색을 위해서 Faiss를 사용하게 된다. 그래서 `faiss.index` 파일이 생기고, `pip list`로 보면 `faiss-cpu`도 설치 되어 있는 것을 확인할 수 있다.
