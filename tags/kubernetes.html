<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><script>!function(){try {var d=document.documentElement.classList;d.remove('light','dark');var e=localStorage.getItem('theme');if("system"===e||(!e&&true)){var t="(prefers-color-scheme: dark)",m=window.matchMedia(t);m.media!==t||m.matches?d.add('dark'):d.add('light')}else if(e) d.add(e)}catch(e){}}()</script><meta content="width=device-width, initial-scale=1" name="viewport"/><link rel="manifest" href="/jayground8/static/favicons/manifest.json"/><link rel="manifest" href="/jayground8/static/favicons/site.webmanifest"/><link rel="shortcut icon" href="/jayground8/static/favicons/favicon.ico"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8773084925406986" crossorigin="anonymous"></script><title>kubernetes - Jay</title><meta name="robots" content="follow, index"/><meta name="description" content="kubernetes tags - Jay"/><meta property="og:url" content="https://jayground8.github.io/tags/kubernetes"/><meta property="og:type" content="website"/><meta property="og:site_name" content="Jayground8"/><meta property="og:description" content="kubernetes tags - Jay"/><meta property="og:title" content="kubernetes - Jay"/><meta property="og:image" content="https://jayground8.github.iosocial-banner.png"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site"/><meta name="twitter:title" content="kubernetes - Jay"/><meta name="twitter:description" content="kubernetes tags - Jay"/><meta name="twitter:image" content="https://jayground8.github.iosocial-banner.png"/><link rel="canonical" href="https://jayground8.github.io/tags/kubernetes"/><link rel="alternate" type="application/rss+xml" title="kubernetes tags - Jay - RSS feed" href="https://jayground8.github.io/tags/kubernetes/feed.xml"/><meta name="next-head-count" content="23"/><link rel="apple-touch-icon" sizes="76x76" href="/static/favicons/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/static/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/static/favicons/favicon-16x16.png"/><link rel="manifest" href="/static/favicons/site.webmanifest"/><link rel="mask-icon" href="/static/favicons/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#000000"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><link rel="preload" href="/_next/static/css/414f98a58444a4f1.css" as="style"/><link rel="stylesheet" href="/_next/static/css/414f98a58444a4f1.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-690364db8bfb1f01.js" defer=""></script><script src="/_next/static/chunks/main-eea1c213342d95c3.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0a1e58e7b4484e05.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Btag%5D-c14d8690282200d8.js" defer=""></script><script src="/_next/static/SBDB4aCCPzRTUvoAVrPlP/_buildManifest.js" defer=""></script><script src="/_next/static/SBDB4aCCPzRTUvoAVrPlP/_ssgManifest.js" defer=""></script><script src="/_next/static/SBDB4aCCPzRTUvoAVrPlP/_middlewareManifest.js" defer=""></script></head><body class="bg-white text-black antialiased dark:bg-gray-900 dark:text-white"><div id="__next" data-reactroot=""><div class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><div class="flex h-screen flex-col justify-between"><header class="flex items-center justify-between py-10"><div><a aria-label="Jayground8" href="/"><div class="flex items-center justify-between"><div class="mr-3"><svg xmlns="http://www.w3.org/2000/svg" width="53.87" height="43.61" viewBox="0 0 512 512" class="fill-black dark:fill-white"><path d="M368.4 17c-8.8 1.3-18.3 5-25.3 9.9-3.8 2.5-22.6 16.1-41.9 30.1-37.1 26.9-38.6 28.3-36.3 33.9 1.7 4 4.9 5.1 14.6 5.1h8.5v128h-17.9c-18.2 0-22.2.6-27.3 4.2l-2.8 1.9V116.9l2.5-2.4c1.5-1.6 3.6-2.5 5.5-2.5s4 .9 5.5 2.5l2.5 2.4v42c0 29.1.3 42.8 1.1 44.5 2.6 5.7 11.5 5.5 13.9-.3.7-1.9 1-15.6.8-46.2-.3-40.1-.4-43.7-2.2-46.9-4.8-9-11.9-13.4-21.6-13.4-13.8 0-22.8 9.1-23.8 24.1l-.5 7.3h-31.5l-1.2-2.9c-1.6-3.9-6-5.8-9.9-4.1-1.6.6-3.5 2.5-4.1 4.1l-1.2 2.9h-63.6l-1.2-2.9c-2.5-6.2-11.5-6.2-14 0l-1.2 2.9H64.3l-.5-7.3c-1-15-10-24.1-23.8-24.1-9.7 0-16.8 4.4-21.6 13.4-1.8 3.3-1.9 7.5-2.2 75-.2 50 .1 72.3.8 74.1 2.1 5.1 8.6 6.3 12.5 2.4l2.5-2.4V116.9l2.5-2.4c3.2-3.3 7.8-3.3 11 0l2.5 2.4V448H32V284.9l-2.5-2.4c-3.8-3.9-10.2-2.7-12.4 2.2-1.6 3.4-1.6 171.2 0 174.6.6 1.4 2.2 3 3.6 3.6 1.6.7 15.6 1.1 43.7 1.1 38 0 41.5-.1 43.4-1.8 1.2-.9 33.4-50.5 71.7-110.2 38.3-59.7 70.5-109.3 71.7-110.3 1.8-1.5 4.3-1.7 19.4-1.7H288v16h-12.5c-13.1 0-16.6.8-18.3 4.4-2 4.5-138.5 216.2-140.4 217.9-1.9 1.6-5.6 1.7-47.9 1.7-31.9 0-46.6.3-48.3 1.1-5.7 2.6-5.5 11.5.3 13.9 1.9.7 16.7 1 50.1.8 46.4-.3 47.6-.3 51.5-2.5 2.2-1.1 5.1-3.3 6.5-4.7 1.3-1.5 10.1-14.7 19.6-29.4l17.1-26.7 18.4-.5c10.1-.3 19.1-.9 20-1.3.9-.5 2.1-1.9 2.8-3.3 1.5-3.4 1.5-35.4 0-38.8-1.1-2.3-4-4-8-4.8-1.3-.2 1.5-5.2 11.5-20.7l13.1-20.5.5 56.5c.3 31 .9 57.1 1.3 58 1.5 3 5.6 3.9 18.2 3.9H256v14c0 14.9.7 18.1 4.7 19.9 1.7.8 34.7 1.1 115.3 1.1s113.6-.3 115.3-1.1c1.4-.6 3-2.2 3.6-3.6 1.5-3.4 1.5-83.2 0-86.6-2.2-4.9-8.6-6.1-12.4-2.2l-2.5 2.4V480H272V272h208v107.1l2.5 2.4c3.8 3.9 10.2 2.7 12.4-2.2 1.5-3.4 1.5-115.2 0-118.6-1.8-3.9-5-4.7-18.4-4.7H464v-40.5c0-27.9-.3-41.2-1.1-42.9-2.6-5.5-11.2-5.5-13.8 0-.8 1.7-1.1 15-1.1 42.9V256h-24v-34.5c0-21.3-.4-36.4-1.1-39.7-3.6-17.3-19.3-32.9-36.7-36.7-16.2-3.4-31.4 1.1-43.3 12.9-7.4 7.3-11.2 13.9-13.4 23.2-1.8 7.9-2.1 50.3-.4 54.2 1.6 3.5 6.1 5.2 9.8 3.7 4.7-2 5.1-4.2 5.1-29.6 0-21.5.2-23.9 2.2-29.2C350.6 168.4 363 160 376 160s25.4 8.4 29.8 20.3c2.1 5.5 2.2 7.4 2.2 40.7v35H304V96h144v24.5c0 16 .4 25.3 1.1 26.9 2.6 5.5 11.2 5.5 13.8 0 .7-1.6 1.1-10.9 1.1-26.9V96h8.5c9.7 0 12.9-1.1 14.6-5.1 2.1-5.2.1-7.5-17.7-20.4C446.2 53.8 445 53 442.1 53c-4.1.1-7.5 3.5-7.5 7.7 0 4 2.2 6.3 14.8 15.3l4.8 3.5-39.1.3c-21.5.1-56.7.1-78.2 0l-39.1-.3 14.9-10.7c8.2-5.9 20.7-15.1 27.8-20.4 14.5-10.8 20.5-14 29.4-15.5 11.6-2.1 22.4 1.3 36.5 11.4 4.3 3.1 9 5.7 10.3 5.7 3.9 0 7.5-4.4 7.1-8.7-.3-3.3-1.1-4.4-7.8-9.6-15.3-11.8-32.3-17.1-47.6-14.7zM96 152v8H64v-16h32v8zm80 0v8h-64v-16h64v8zm48 0v8h-32v-16h32v8zM96 280v104h-4.5c-5.5 0-9 1.5-10.4 4.7-1.5 3.3-1.5 35.4 0 38.7 1.7 3.6 3.1 4 16.2 4.6l11.9.5-5.1 7.7L99 448H64V176h32v104zm80-27.8v76.3l-17.8 27.7-17.7 27.7-14.2.1H112V176h64v76.2zm48-37.3v38.9l-15.6 24.4c-8.6 13.3-15.8 24.5-16 24.7-.2.2-.4-28.2-.4-63.2V176h32v38.9zM256 369v76h-16V317.1l7.8-12c4.2-6.7 7.8-12.1 8-12.1.1 0 .2 34.2.2 76zm-129.1 34.7c-1.2 2.1-3.6 5.7-5.1 8l-2.9 4.3H96v-16h33.2l-2.3 3.7zM192 408v8h-15.1l3.7-5.8c2-3.1 4.4-6.7 5.2-8 .8-1.2 2.4-2.2 3.8-2.2 2.3 0 2.4.2 2.4 8z"></path><path d="M322.5 114.5c-1.6 1.5-2.5 3.6-2.5 5.5s.9 4 2.5 5.5c1.5 1.6 3.6 2.5 5.5 2.5s4-.9 5.5-2.5c1.6-1.5 2.5-3.6 2.5-5.5s-.9-4-2.5-5.5c-1.5-1.6-3.6-2.5-5.5-2.5s-4 .9-5.5 2.5zM418.5 114.5c-1.6 1.5-2.5 3.6-2.5 5.5s.9 4 2.5 5.5c1.5 1.6 3.6 2.5 5.5 2.5s4-.9 5.5-2.5c1.6-1.5 2.5-3.6 2.5-5.5s-.9-4-2.5-5.5c-1.5-1.6-3.6-2.5-5.5-2.5s-4 .9-5.5 2.5zM290.2 290.3c-5 5.3-1.3 13.7 5.9 13.7 4.7 0 7.9-3.4 7.9-8.1 0-7.1-9-10.7-13.8-5.6zM450.5 290.5c-5 4.9-1.5 13.5 5.5 13.5 4.1 0 8-3.9 8-8s-3.9-8-8-8c-1.9 0-4 .9-5.5 2.5zM358.1 322c-8.3 2.2-16.5 5.7-19.8 8.6-3.3 2.8-3.9 7.3-1.4 10.6 3.3 4.1 6.4 4.3 13.8.8 23.3-10.9 47.3-7 64.8 10.5 11.5 11.4 16.5 23.6 16.5 39.7 0 15.9-5.1 28.1-16.4 39.4C404.2 443 392.1 448 376 448s-28.1-5-39.5-16.5c-11-11.1-16.5-24.2-16.5-39.5.1-9.5 1.2-14.5 5.6-24.3 3.8-8.3 3.8-10.4.1-14.5-4.7-5.1-10.8-1.7-15.4 8.6-8.6 19.5-8.4 42.7.6 61.7 9.1 19.3 28.3 34.5 49.5 39 11.9 2.6 28.4 1.7 39.3-2.2 23.9-8.6 41.3-27.9 46.9-51.9 1.9-8.5 1.9-24.4-.1-33.1-5.9-25.5-26.7-46.6-52.5-53.3-10.1-2.6-25.8-2.6-35.9 0zM290.2 450.3c-5 5.3-1.3 13.7 5.9 13.7 4.7 0 7.9-3.4 7.9-8.1 0-7.1-9-10.7-13.8-5.6zM450.2 450.3c-5 5.3-1.3 13.7 5.9 13.7 4.7 0 7.9-3.4 7.9-8.1 0-7.1-9-10.7-13.8-5.6z"></path></svg></div><div class="hidden h-6 text-2xl font-semibold sm:block">Jayground8</div></div></a></div><div class="flex items-center text-base leading-5"><div class="hidden sm:block"><a class="p-1 font-medium text-gray-900 dark:text-gray-100 sm:p-4" href="/blog">블로그</a><a class="p-1 font-medium text-gray-900 dark:text-gray-100 sm:p-4" href="/tags">Tags</a></div><button aria-label="Toggle Dark Mode" type="button" class="ml-1 mr-1 h-8 w-8 rounded p-1 sm:ml-4"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001 0 1010.586 10.586z"></path></svg></button><div class="sm:hidden"><button type="button" class="ml-1 mr-1 h-8 w-8 rounded py-1" aria-label="Toggle Menu"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button><div class="fixed top-0 left-0 z-10 h-full w-full transform bg-gray-200 opacity-95 duration-300 ease-in-out dark:bg-gray-800 translate-x-full"><div class="flex justify-end"><button type="button" class="mr-5 mt-11 h-8 w-8 rounded" aria-label="Toggle Menu"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></button></div><nav class="fixed mt-8 h-full"><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/blog">블로그</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/tags">Tags</a></div></nav></div></div></div></header><main class="mb-auto"><div class="divide-y divide-gray-200 dark:divide-gray-700"><div class="space-y-2 pt-6 pb-8 md:space-y-5"><h1 class="text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-6xl md:leading-14">Kubernetes</h1><div class="relative max-w-lg"><input type="text" aria-label="Search articles" placeholder="검색" class="block w-full rounded-md border border-gray-300 bg-white px-4 py-2 text-gray-900 focus:border-primary-500 focus:ring-primary-500 dark:border-gray-900 dark:bg-gray-800 dark:text-gray-100"/><svg class="absolute right-3 top-3 h-5 w-5 text-gray-400 dark:text-gray-300" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></div></div><ul><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2024-05-01T00:00:00.000Z">2024년 5월 1일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20240501-opentelemetry-syslog">OpenTelmetry로 auth.log와 syslog 수집하기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/opentelemetry">opentelemetry</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/devsecops">devsecops</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Ubuntu 20.04 서버의 auth.log, syslog 로그 값들을 OpenTelemetry를 통해서 수집하고 싶었다. 처음에는 Filelog Receiver를 통해서 수집하려고 하였고, rsyslog의 설정값을 변경하여 Filelog로 수집하도록 구성했다. 그런데 이후에 Syslog Receiver가 존재하는 것을 확인하게 되었고, 훨씬 간단하게 syslog를 수집할 수 있었다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2024-04-20T00:00:00.000Z">2024년 4월 20일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20240420-kubelet-imagegc">Kubelet imageGC</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/ncloud">ncloud</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">사이즈가 큰 컨테이너 이미지를 내려받기 위해서 Containerd의 설정값 root을 새로운 스토리지를 추가한 경로로 수정하였다. Kubelet은 imageGCHighThresholdPercent로 설정된 임계치보다 disk 사용량이 많으면, 컨테이너 이미지를 정리하여 disk 공간을 확보하려고 한다. imageGCHighThresholdPercent이 기본값으로 85로 설정되어 있고, disk의 85 퍼센트 이상 사용했을 때 정리 프로세스가 실행된다. 문제는 Containerd root 경로의 volume이 거의 꽉 차더라도, 전체 volume의 사용량은 85 퍼센트가 되지 않을 수 있다는 것이다. Containerd root 경로의 volume에 여유가 없어서 새로 스케쥴된 Pod의 컨테이너 이미지를 내려 받지 못하는 문제가 발생할 수 있다. imageGCHighThresholdPercent 설정값을 조절하여 이 문제를 해결할 수 있다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2024-04-20T00:00:00.000Z">2024년 4월 20일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20240420-loki-multi-tenants">Loki multi-tenants</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Loki로 Log를 수집하고, Grafana로 Log를 보여줄 때 Grafana user별로 볼 수 있는 Log를 제한하고 싶었다. Grafana Enterpirse의 경우에는 Label-based access control을 제공하여, Loki label별로 Query할 수 있는 권한을 제한할 수 있는 것처럼 보인다. 하지만 아쉽게 오픈소스에서는 해당 기능을 제공하지 않는다. 그래서 Loki multi tenant를 통해서 Log를 tenent별로 그룹핑하고, tenant별로 query하는 방법을 사용했다. 그리고 Loki는 인증 layer가 존재하지 않기 때문에 Nginx를 통해서 인증을 하여 query할 수 있도록 하였다. 마지막으로 Network Policy로 Nginx 인증을 통해서 Loki에 접근하도록 강제하여 원하는 구성을 할 수 있었다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2024-03-24T00:00:00.000Z">2024년 3월 24일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20240324-k8s-device-plugin">k8s-device-plugin으로 NVIDIA GPU Multi-Process Service 사용하기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/ncloud">ncloud</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Kubernetes에서 k8s-device-plugin을 통해서 NVIDIA GPU 자원을 쉽게 사용할 수 있다. 하나의 어플리케이션에서 GPU 자원을 온전히 사용하지 못하고 낭비될 때, Time slicing, MPS, MIG 등을 사용하여 여러 프로세스가 GPU 자원을 공유하도록 설정할 수 있다. 네이버 공공 클라우드에서 Tesla T4와 Telsa V100을 제공한다. 해당 GPU 아키텍쳐가 MIG을 지원히지 않기 때문에, MPS를 적용하여 GPU 자원을 공유하는 것을 고려하였다. 최근에 release된 k8s-device-plugin v0.15.0-rc.1부터 MPS가 지원되기 시작되어 해당 버전으로 MPS를 설정해보았다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2024-02-14T00:00:00.000Z">2024년 2월 14일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20240214-containerd-LimitNOFILE">containerd nofile default 설정값</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/containerd">containerd</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Kubernetes에서 pod가 실행될 때, 해당 process의 open file 갯수 limit이 어떻게 되는지 궁금하였다. Containerd가 systemd service로 동작하는 상황에서 systemd에서 LimitNOFILE이 inifinity로 설정되어 있는 것을 확인하였다. infinity가 Ubuntu systemd 240 버전 이상에서는 process에 최대로 할당할 수 있는 File 갯수인 fs.nr_open 값으로 설정된다. 실제로 test용 pod를 띄어서 containerd에 의해서 실행되는 process의 nofile limit이 LimitNOFILE에 설정된 값으로 동일하게 설정되는 것을 확인하였다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2024-02-11T00:00:00.000Z">2024년 2월 11일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20240208-tekton-chain">Tekton Chain</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/tekton">tekton</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/devsecops">devsecops</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Kubernetes에서 Tekton Chain을 통해서 어떻게 Software Supply Chain Security를 구성할 수 있는지 확인했다. Tekton Pipeline으로 git clone을 하고, container image를 build하고, 최종적으로 OCI registry에 push하도록 구성했다. 그리고 Tekton Chain이 어떻게 in-toto spec의 Attestation 정보를 남기는지 확인하였다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2024-02-07T00:00:00.000Z">2024년 2월 7일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20240207-cilium-host-port">Cilium CNI와 HostPort</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/cilium">cilium</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">네이버 클라우드 쿠버네티스 서비스에서 HostPort를 설정하였는데, 정상적으로 Node Port를 통해서 Container에 접근할 수가 없었다. 처음에는 CNI plugin portmap을 설정하여 HostPort를 Iptables Rule로 적용하도록 하였다. 하지만 현재 운영하는 Kernel 버전이 kube proxy를 대체할 수 있는 것을 파악하였고, default로 설정된 KubeProxyReplacement=disabled을 KubeProxyReplacement=strict으로 설정하여 Cilium eBPF로 대체하여 사용하였다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2024-01-24T00:00:00.000Z">2024년 1월 24일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20240124-cert-manager-webhook">Cert Manager Webhook 작성하여 Naver Cloud에서 DNS-01 challeng로 Lets Encrypt 인증서 발급하기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/ncloud">ncloud</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Cert Manager의 DNS provider 목록에 Naver Cloud DNS는 없었다. 하지만 Cert Manager에서 새로운 DNS provider를 직접 연결해서 사용할 수 있도록 webhook solver라는 것을 제공한다. 따라서 Naver Cloud API를 사용하여 webhook 코드를 작성하고, Github Action을 통해서 Image와 Helm chart를 배포해서 사용했다. Cert Manager에서 Boilerplate code와 test framework를 제공하여 비교적 쉽게 작성해서 사용할 수 있었다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-12-31T00:00:00.000Z">2023년 12월 31일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20231231-open-telemetry">OpenTelemetry with Minikube</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/opentelemetry">opentelemetry</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">OpenTelemetry collector를 사용하여 tracing과 logging을 같이 하는 것을 검토하였다. Kubernetes와 멀어져 있던 사이에 OpenTelemetry 커뮤니티가 엄청나게 성장한 것을 깨닫게 되었다. Log에 traceID를 남기고 그걸로 Tracing 정보를 볼 수 있도록 구성했고, Grafana 하나에서 통합적으로 볼 수 있도록 Loki와 Tempo를 Exporter로 사용했다. 아직 Python과 Nodejs에서는 Log쪽의 상태는 Development나 Experimental이기 때문에, receiver에서 filelog를 사용하여 Kubernetes log file을 fluentbit처럼 tail해서 가져오고 traceId를 log에 넣어주는 instrument libary를 사용했다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-12-22T00:00:00.000Z">2023년 12월 22일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20231222-vault-on-k8s">vault secrets operator 사용해보기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">Kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/vault">vault</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">GitOps에서 Secret을 어떻게 관리할지 고민을 하였고, 개발자들의 인지부하를 줄이기 위해서 Vault UI로 자신의 앱의 비밀값을 관리하는 것이 제일 효율적이라는 판단을 했다. Vault secrets operator가 GA로 공유가 되었고, Secrets Store CSI나 External secrets 프로젝트보다 깔끔한 방식이라는 생각이 들었다. Vault secrets operator의 CRD로 vault secret과 kubernetes secret의 sync를 맞추고, reloader로 secret이 변경되었을 때 다시 pod를 배포하는 것을 테스트해보았다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-12-14T00:00:00.000Z">2023년 12월 14일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20231214-argocd-notification">Argo CD Notification으로 간단하게 Slack 알림 보내기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">Kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/argocd">ArgoCD</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Argo CD Notification가 이미 있어서 쉽게 Slack 알림을 연동할 수 있다. 메뉴얼에서 친절하게 셋팅 방법을 설명하고 있지만, 처음 보고 셋팅할 때 놓칠 수 있는 부분을 기록으로 남긴다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-12-05T00:00:00.000Z">2023년 12월 5일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20231205-nginx-allow-ip">Kubernetes에서 Nginx로 IP allow list 제어</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">Kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/nginx">Nginx</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Kubernetes cluster에서 Nginx로 허용 가능한 IP를 설정하고 싶었다. 간단하게 끝날 줄 알았던 작업은 또다른 삽질기가 되었다.😭 Kubernetes에서 SNAT이 되는 과정을 이해하고 externalTrafficPolicy를 Local로 설정했다. 그리고 External Load balancer에서는 client IP를 전달하기 위해서 Proxy Protocol와 같은 것을 설정하고, Nginx에서 그 Proxy protocol로 전달된 IP address로 Access control을 할 수 있도록 설정하였다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-10-29T00:00:00.000Z">2023년 10월 29일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20231029-gpu-on-k8s">Kubernetes에서 GPU 사용하기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Pytorch를 컨테이너로 띄우고 GPU를 사용하고 싶었다. 따라서 Docker container에서 NVIDIA GPU를 사용할 수 있도록 셋팅을 해보았다. 그리고 최종적으로 Kubernetes에서 GPU hardware를 사용할 수 있도록 nvidia device plugin을 DaemonSet으로 띄우고 Pod를 실행해보았다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-10-21T00:00:00.000Z">2023년 10월 21일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20231021-csi-driver">aws-ebs-csi-driver 코드로 csi driver 이해하기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">aws-ebs-csi-driver 소스코드를 보고 csi driver가 하는 역할을 이해해보았다. 크게 Controller plugin과 Node plugin으로 구성되어 있고, 다양한 sidecar container들이 존재한다. 이 sidecar container들이 Volume을 생성하고, 노드에 부착하고, Host 혹은 container directory에 mount하는 것을 진행한다. 이과정에서 CSI driver로 cloud vendor마다 다른 로직으로 그들의 volume을 제어하게 된다. 네이버 클라우드에서 storage가 처음 생성될 때 서버에 부착이 되어야만 하는 제약사항이 있어서, PVC로 PV 동적 할당을 할 때 volumeBindingMode이 Immediate이면 자동으로 Node에 부착이 된다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-09-28T00:00:00.000Z">2023년 9월 28일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20230928-ecr-credential-provider">ECR Credential-Provider 사용해보기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Kubeadm으로 만든 Kubernetes Cluster에서 AWS ECR를 private container image repository를 사용하고자 하였다. private repository를 접근하기 위해서 Kubernetes 1.26부터 stable feature로 제공하는 kubelet crendential provider를 사용하였다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-07-30T00:00:00.000Z">2023년 7월 30일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20230730-k8s-controller-external-secrets">오픈소스 Kubernetes External Secrets 살펴보기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">CRD와 Custom Controller를 사용하는 프로젝트에 대해서 더 알고 싶은 상황에서, External Secrets이라는 프로젝트를 발견하게 되었다. External Secrets는 Vault, AWS secret manager와 같이 secret 관리하는 Tool들과 Kubernetes secret을 CRD을 통해서 sync할 수 있게 해준다. 이 프로젝트는 kubebuilder를 사용하였는데, 소스코드를 보면서 어떻게 구현한 건지 자세히 살펴보았다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-07-22T00:00:00.000Z">2023년 7월 22일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20230722-calico-kube-controller">Calico kube-controller 이해하기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/calico">calico</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Calico에서 어떻게 CRD를 활용하는지 이해하기 위해서 Calico Opensource 버전의 깃헙 소스코드를 살펴보게 되었다. Calico archiecture에서 kube-controller 부분이 어떤 역할을 하는지 소스 코드를 통해서 이해할 수 있게 되었다. kube-controller들은 kubernetes native resource에 대한 변경을 calico data store와 sync해주는 역할을 하고 있다. 내가 사용하는 Minikbue Kubernetes Cluster에서는 Calico의 data store는 kubernetes로 설정되어 있기 때문에, CRD로 Calico data들이 저장되고 Felix가 이것을 watch하여 변화에 대해서 network rule을 업데이트 하게 된다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-07-16T00:00:00.000Z">2023년 7월 16일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20230716-if-kakao-2022-k8s-controller">if(kakao)2022 Testing Kubernetes Controller 발표 따라 만들기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">어제는 Programming Kubernetes에 나오는 예제를 kubebuilder로 작성해보았다. if(kakao)2022에서 controller를 테스트하는 방법에 대해서 설명한 발표가 있었다. 어제 이해한 내용을 바탕으로 이 발표에서 사용한 BlueGreen controller 예제를 따라서 작성해보았다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-07-15T00:00:00.000Z">2023년 7월 15일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20230715-k8s-controller">Kubernetes custom controller 작성해보기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">Programming Kubernetes 책에서 나온 예제를 따라서 custom controller를 작성해보았다. CustomResourceDefinition를 어떻게 정의하고, kubebuilder로 어떻게 나만의 business logic을 작성할 수 있는지 예제를 통해서 이해했다.</div></div></article></li><li class="py-4"><article class="space-y-2 xl:grid xl:grid-cols-4 xl:items-baseline xl:space-y-0"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-07-03T00:00:00.000Z">2023년 7월 3일</time></dd></dl><div class="space-y-3 xl:col-span-3"><div><h3 class="text-2xl font-bold leading-8 tracking-tight"><a class="text-gray-900 dark:text-gray-100" href="/blog/20230703-k8s-community-days">흑역사 하나 더 만들기</a></h3><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/kubernetes">kubernetes</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/presentation">presentation</a></div></div><div class="prose max-w-none text-gray-500 dark:text-gray-400">오늘 대왕 흑역사를 생산했다. Kubernetes Community Days에서 발표를 하게 되었는데, 30분 발표시간에 맞춰서 발표자료를 잘 준비하지 못해서 부끄러웠다. 30분에 너무 많은 내용을 담을려고 했고, 라이브코딩 형식으로 해서 시간도 많이 걸렸다. 나는 발표를 왜 할까?</div></div></article></li></ul></div></main><footer><div class="mt-16 flex flex-col items-center"><div class="mb-3 flex space-x-4"><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="mailto:jayground8@gmail.com"><span class="sr-only">mail</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="fill-current text-gray-700 hover:text-blue-500 dark:text-gray-200 dark:hover:text-blue-400 h-6 w-6"><path d="M2.003 5.884 10 9.882l7.997-3.998A2 2 0 0 0 16 4H4a2 2 0 0 0-1.997 1.884z"></path><path d="m18 8.118-8 4-8-4V14a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8.118z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://github.com/jayground8"><span class="sr-only">github</span><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="fill-current text-gray-700 hover:text-blue-500 dark:text-gray-200 dark:hover:text-blue-400 h-6 w-6"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a></div><div class="mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400"><div>Jay</div><div> • </div><div>© 2024</div><div> • </div><a href="/">Jayground8</a></div><div class="mb-8 text-sm text-gray-500 dark:text-gray-400"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timlrx/tailwind-nextjs-starter-blog">Tailwind Nextjs Theme</a></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"OpenTelmetry로 auth.log와 syslog 수집하기","date":"2024-05-01T00:00:00.000Z","tags":["kubernetes","opentelemetry","devsecops"],"images":["/static/images/social-banner.png"],"summary":"Ubuntu 20.04 서버의 auth.log, syslog 로그 값들을 OpenTelemetry를 통해서 수집하고 싶었다. 처음에는 Filelog Receiver를 통해서 수집하려고 하였고, rsyslog의 설정값을 변경하여 Filelog로 수집하도록 구성했다. 그런데 이후에 Syslog Receiver가 존재하는 것을 확인하게 되었고, 훨씬 간단하게 syslog를 수집할 수 있었다.","slug":"20240501-opentelemetry-syslog"},{"title":"Kubelet imageGC","date":"2024-04-20T00:00:00.000Z","tags":["kubernetes","ncloud"],"images":["/static/images/social-banner.png"],"summary":"사이즈가 큰 컨테이너 이미지를 내려받기 위해서 Containerd의 설정값 root을 새로운 스토리지를 추가한 경로로 수정하였다. Kubelet은 imageGCHighThresholdPercent로 설정된 임계치보다 disk 사용량이 많으면, 컨테이너 이미지를 정리하여 disk 공간을 확보하려고 한다. imageGCHighThresholdPercent이 기본값으로 85로 설정되어 있고, disk의 85 퍼센트 이상 사용했을 때 정리 프로세스가 실행된다. 문제는 Containerd root 경로의 volume이 거의 꽉 차더라도, 전체 volume의 사용량은 85 퍼센트가 되지 않을 수 있다는 것이다. Containerd root 경로의 volume에 여유가 없어서 새로 스케쥴된 Pod의 컨테이너 이미지를 내려 받지 못하는 문제가 발생할 수 있다. imageGCHighThresholdPercent 설정값을 조절하여 이 문제를 해결할 수 있다.","slug":"20240420-kubelet-imagegc"},{"title":"Loki multi-tenants","date":"2024-04-20T00:00:00.000Z","tags":["kubernetes"],"images":["/static/images/social-banner.png"],"summary":"Loki로 Log를 수집하고, Grafana로 Log를 보여줄 때 Grafana user별로 볼 수 있는 Log를 제한하고 싶었다. Grafana Enterpirse의 경우에는 Label-based access control을 제공하여, Loki label별로 Query할 수 있는 권한을 제한할 수 있는 것처럼 보인다. 하지만 아쉽게 오픈소스에서는 해당 기능을 제공하지 않는다. 그래서 Loki multi tenant를 통해서 Log를 tenent별로 그룹핑하고, tenant별로 query하는 방법을 사용했다. 그리고 Loki는 인증 layer가 존재하지 않기 때문에 Nginx를 통해서 인증을 하여 query할 수 있도록 하였다. 마지막으로 Network Policy로 Nginx 인증을 통해서 Loki에 접근하도록 강제하여 원하는 구성을 할 수 있었다.","slug":"20240420-loki-multi-tenants"},{"title":"k8s-device-plugin으로 NVIDIA GPU Multi-Process Service 사용하기","date":"2024-03-24T00:00:00.000Z","tags":["kubernetes","ncloud"],"images":["/static/images/social-banner.png"],"summary":"Kubernetes에서 k8s-device-plugin을 통해서 NVIDIA GPU 자원을 쉽게 사용할 수 있다. 하나의 어플리케이션에서 GPU 자원을 온전히 사용하지 못하고 낭비될 때, Time slicing, MPS, MIG 등을 사용하여 여러 프로세스가 GPU 자원을 공유하도록 설정할 수 있다. 네이버 공공 클라우드에서 Tesla T4와 Telsa V100을 제공한다. 해당 GPU 아키텍쳐가 MIG을 지원히지 않기 때문에, MPS를 적용하여 GPU 자원을 공유하는 것을 고려하였다. 최근에 release된 k8s-device-plugin v0.15.0-rc.1부터 MPS가 지원되기 시작되어 해당 버전으로 MPS를 설정해보았다.","slug":"20240324-k8s-device-plugin"},{"title":"containerd nofile default 설정값","date":"2024-02-14T00:00:00.000Z","tags":["kubernetes","containerd"],"images":["/static/images/social-banner.png"],"summary":"Kubernetes에서 pod가 실행될 때, 해당 process의 open file 갯수 limit이 어떻게 되는지 궁금하였다. Containerd가 systemd service로 동작하는 상황에서 systemd에서 LimitNOFILE이 inifinity로 설정되어 있는 것을 확인하였다. infinity가 Ubuntu systemd 240 버전 이상에서는 process에 최대로 할당할 수 있는 File 갯수인 fs.nr_open 값으로 설정된다. 실제로 test용 pod를 띄어서 containerd에 의해서 실행되는 process의 nofile limit이 LimitNOFILE에 설정된 값으로 동일하게 설정되는 것을 확인하였다.","slug":"20240214-containerd-LimitNOFILE"},{"title":"Tekton Chain","date":"2024-02-11T00:00:00.000Z","tags":["kubernetes","tekton","devsecops"],"images":["/static/images/social-banner.png"],"summary":"Kubernetes에서 Tekton Chain을 통해서 어떻게 Software Supply Chain Security를 구성할 수 있는지 확인했다. Tekton Pipeline으로 git clone을 하고, container image를 build하고, 최종적으로 OCI registry에 push하도록 구성했다. 그리고 Tekton Chain이 어떻게 in-toto spec의 Attestation 정보를 남기는지 확인하였다.","slug":"20240208-tekton-chain"},{"title":"Cilium CNI와 HostPort","date":"2024-02-07T00:00:00.000Z","tags":["kubernetes","cilium"],"images":["/static/images/social-banner.png"],"summary":"네이버 클라우드 쿠버네티스 서비스에서 HostPort를 설정하였는데, 정상적으로 Node Port를 통해서 Container에 접근할 수가 없었다. 처음에는 CNI plugin portmap을 설정하여 HostPort를 Iptables Rule로 적용하도록 하였다. 하지만 현재 운영하는 Kernel 버전이 kube proxy를 대체할 수 있는 것을 파악하였고, default로 설정된 KubeProxyReplacement=disabled을 KubeProxyReplacement=strict으로 설정하여 Cilium eBPF로 대체하여 사용하였다.","slug":"20240207-cilium-host-port"},{"title":"Cert Manager Webhook 작성하여 Naver Cloud에서 DNS-01 challeng로 Lets Encrypt 인증서 발급하기","date":"2024-01-24T00:00:00.000Z","tags":["kubernetes","ncloud"],"images":["/static/images/social-banner.png"],"summary":"Cert Manager의 DNS provider 목록에 Naver Cloud DNS는 없었다. 하지만 Cert Manager에서 새로운 DNS provider를 직접 연결해서 사용할 수 있도록 webhook solver라는 것을 제공한다. 따라서 Naver Cloud API를 사용하여 webhook 코드를 작성하고, Github Action을 통해서 Image와 Helm chart를 배포해서 사용했다. Cert Manager에서 Boilerplate code와 test framework를 제공하여 비교적 쉽게 작성해서 사용할 수 있었다.","slug":"20240124-cert-manager-webhook"},{"title":"OpenTelemetry with Minikube","date":"2023-12-31T00:00:00.000Z","tags":["opentelemetry","kubernetes"],"images":["/static/images/social-banner.png"],"summary":"OpenTelemetry collector를 사용하여 tracing과 logging을 같이 하는 것을 검토하였다. Kubernetes와 멀어져 있던 사이에 OpenTelemetry 커뮤니티가 엄청나게 성장한 것을 깨닫게 되었다. Log에 traceID를 남기고 그걸로 Tracing 정보를 볼 수 있도록 구성했고, Grafana 하나에서 통합적으로 볼 수 있도록 Loki와 Tempo를 Exporter로 사용했다. 아직 Python과 Nodejs에서는 Log쪽의 상태는 Development나 Experimental이기 때문에, receiver에서 filelog를 사용하여 Kubernetes log file을 fluentbit처럼 tail해서 가져오고 traceId를 log에 넣어주는 instrument libary를 사용했다.","slug":"20231231-open-telemetry"},{"title":"vault secrets operator 사용해보기","date":"2023-12-22T00:00:00.000Z","tags":["Kubernetes","vault"],"images":["/static/images/social-banner.png"],"summary":"GitOps에서 Secret을 어떻게 관리할지 고민을 하였고, 개발자들의 인지부하를 줄이기 위해서 Vault UI로 자신의 앱의 비밀값을 관리하는 것이 제일 효율적이라는 판단을 했다. Vault secrets operator가 GA로 공유가 되었고, Secrets Store CSI나 External secrets 프로젝트보다 깔끔한 방식이라는 생각이 들었다. Vault secrets operator의 CRD로 vault secret과 kubernetes secret의 sync를 맞추고, reloader로 secret이 변경되었을 때 다시 pod를 배포하는 것을 테스트해보았다.","slug":"20231222-vault-on-k8s"},{"title":"Argo CD Notification으로 간단하게 Slack 알림 보내기","date":"2023-12-14T00:00:00.000Z","tags":["Kubernetes","ArgoCD"],"images":["/static/images/social-banner.png"],"summary":"Argo CD Notification가 이미 있어서 쉽게 Slack 알림을 연동할 수 있다. 메뉴얼에서 친절하게 셋팅 방법을 설명하고 있지만, 처음 보고 셋팅할 때 놓칠 수 있는 부분을 기록으로 남긴다.","slug":"20231214-argocd-notification"},{"title":"Kubernetes에서 Nginx로 IP allow list 제어","date":"2023-12-05T00:00:00.000Z","tags":["Kubernetes","Nginx"],"images":["/static/images/social-banner.png"],"summary":"Kubernetes cluster에서 Nginx로 허용 가능한 IP를 설정하고 싶었다. 간단하게 끝날 줄 알았던 작업은 또다른 삽질기가 되었다.😭 Kubernetes에서 SNAT이 되는 과정을 이해하고 externalTrafficPolicy를 Local로 설정했다. 그리고 External Load balancer에서는 client IP를 전달하기 위해서 Proxy Protocol와 같은 것을 설정하고, Nginx에서 그 Proxy protocol로 전달된 IP address로 Access control을 할 수 있도록 설정하였다.","slug":"20231205-nginx-allow-ip"},{"title":"Kubernetes에서 GPU 사용하기","date":"2023-10-29T00:00:00.000Z","tags":["kubernetes"],"images":["/static/images/social-banner.png"],"summary":"Pytorch를 컨테이너로 띄우고 GPU를 사용하고 싶었다. 따라서 Docker container에서 NVIDIA GPU를 사용할 수 있도록 셋팅을 해보았다. 그리고 최종적으로 Kubernetes에서 GPU hardware를 사용할 수 있도록 nvidia device plugin을 DaemonSet으로 띄우고 Pod를 실행해보았다.","slug":"20231029-gpu-on-k8s"},{"title":"aws-ebs-csi-driver 코드로 csi driver 이해하기","date":"2023-10-21T00:00:00.000Z","tags":["kubernetes"],"images":["/static/images/social-banner.png"],"summary":"aws-ebs-csi-driver 소스코드를 보고 csi driver가 하는 역할을 이해해보았다. 크게 Controller plugin과 Node plugin으로 구성되어 있고, 다양한 sidecar container들이 존재한다. 이 sidecar container들이 Volume을 생성하고, 노드에 부착하고, Host 혹은 container directory에 mount하는 것을 진행한다. 이과정에서 CSI driver로 cloud vendor마다 다른 로직으로 그들의 volume을 제어하게 된다. 네이버 클라우드에서 storage가 처음 생성될 때 서버에 부착이 되어야만 하는 제약사항이 있어서, PVC로 PV 동적 할당을 할 때 volumeBindingMode이 Immediate이면 자동으로 Node에 부착이 된다.","slug":"20231021-csi-driver"},{"title":"ECR Credential-Provider 사용해보기","date":"2023-09-28T00:00:00.000Z","tags":["kubernetes"],"summary":"Kubeadm으로 만든 Kubernetes Cluster에서 AWS ECR를 private container image repository를 사용하고자 하였다. private repository를 접근하기 위해서 Kubernetes 1.26부터 stable feature로 제공하는 kubelet crendential provider를 사용하였다.","slug":"20230928-ecr-credential-provider"},{"title":"오픈소스 Kubernetes External Secrets 살펴보기","date":"2023-07-30T00:00:00.000Z","tags":["kubernetes"],"summary":"CRD와 Custom Controller를 사용하는 프로젝트에 대해서 더 알고 싶은 상황에서, External Secrets이라는 프로젝트를 발견하게 되었다. External Secrets는 Vault, AWS secret manager와 같이 secret 관리하는 Tool들과 Kubernetes secret을 CRD을 통해서 sync할 수 있게 해준다. 이 프로젝트는 kubebuilder를 사용하였는데, 소스코드를 보면서 어떻게 구현한 건지 자세히 살펴보았다.","slug":"20230730-k8s-controller-external-secrets"},{"title":"Calico kube-controller 이해하기","date":"2023-07-22T00:00:00.000Z","tags":["kubernetes","calico"],"summary":"Calico에서 어떻게 CRD를 활용하는지 이해하기 위해서 Calico Opensource 버전의 깃헙 소스코드를 살펴보게 되었다. Calico archiecture에서 kube-controller 부분이 어떤 역할을 하는지 소스 코드를 통해서 이해할 수 있게 되었다. kube-controller들은 kubernetes native resource에 대한 변경을 calico data store와 sync해주는 역할을 하고 있다. 내가 사용하는 Minikbue Kubernetes Cluster에서는 Calico의 data store는 kubernetes로 설정되어 있기 때문에, CRD로 Calico data들이 저장되고 Felix가 이것을 watch하여 변화에 대해서 network rule을 업데이트 하게 된다.","slug":"20230722-calico-kube-controller"},{"title":"if(kakao)2022 Testing Kubernetes Controller 발표 따라 만들기","date":"2023-07-16T00:00:00.000Z","tags":["kubernetes"],"summary":"어제는 Programming Kubernetes에 나오는 예제를 kubebuilder로 작성해보았다. if(kakao)2022에서 controller를 테스트하는 방법에 대해서 설명한 발표가 있었다. 어제 이해한 내용을 바탕으로 이 발표에서 사용한 BlueGreen controller 예제를 따라서 작성해보았다.","slug":"20230716-if-kakao-2022-k8s-controller"},{"title":"Kubernetes custom controller 작성해보기","date":"2023-07-15T00:00:00.000Z","tags":["kubernetes"],"summary":"Programming Kubernetes 책에서 나온 예제를 따라서 custom controller를 작성해보았다. CustomResourceDefinition를 어떻게 정의하고, kubebuilder로 어떻게 나만의 business logic을 작성할 수 있는지 예제를 통해서 이해했다.","slug":"20230715-k8s-controller"},{"title":"흑역사 하나 더 만들기","date":"2023-07-03T00:00:00.000Z","tags":["kubernetes","presentation"],"summary":"오늘 대왕 흑역사를 생산했다. Kubernetes Community Days에서 발표를 하게 되었는데, 30분 발표시간에 맞춰서 발표자료를 잘 준비하지 못해서 부끄러웠다. 30분에 너무 많은 내용을 담을려고 했고, 라이브코딩 형식으로 해서 시간도 많이 걸렸다. 나는 발표를 왜 할까?","slug":"20230703-k8s-community-days"}],"tag":"kubernetes"},"__N_SSG":true},"page":"/tags/[tag]","query":{"tag":"kubernetes"},"buildId":"SBDB4aCCPzRTUvoAVrPlP","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>